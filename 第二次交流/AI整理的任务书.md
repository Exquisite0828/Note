## 研究主题：路径规划算法比较与应用

### 核心算法对比

**A\* vs RRT算法评估**

- 在小规模地图和大规模地图上的适用性比较  
- 在障碍物多或少情况下的适用性比较
- 各自的优势和劣势分析   
- 是否能保证最优路径的问题   



- 算法变体的效率提升情况（暂时未知）



## 小规模地图 vs 大规模地图的适用性

在小规模地图上，A算法通常表现更优。由于搜索空间相对较小，A*的启发式搜索能够快速找到最优路径，时间复杂度和空间复杂度都在可接受范围内。A在小规模环境中的确定性和最优性保证使其成为首选。

对于大规模地图，RRT算法展现出明显优势。随着地图规模增长，A*的内存需求呈指数级增长，而RRT只需要维护一个树结构，内存使用相对稳定。RRT的概率完备性意味着在给定足够时间的情况下，总能找到解决方案，这在高维空间中尤为重要。

## 障碍物密度对算法性能的影响

在障碍物稀少的开阔环境中，A算法能够充分发挥其启发式搜索的优势，快速朝目标方向搜索。由于障碍物较少，搜索树相对简单，A*通常能在较短时间内找到最优路径.

当障碍物密集时，A需要在复杂的障碍物间隙中搜索可行路径，可能需要探索大量节点。而RRT算法在这种情况下表现相对稳定，因为它通过随机采样来探索自由空间，不会被复杂的障碍物布局困扰，能够灵活地"绕过"障碍物群。

## 算法优势与劣势分析

A算法的主要优势在于其完备性和最优性保证。它能够系统地搜索整个状态空间，确保找到代价最小的路径。算法具有良好的理论基础，行为可预测，适合对路径质量要求较高的应用。然而，A*的劣势也很明显：在高维空间中存在维数灾难问题，内存需求可能非常庞大，且在动态环境中需要重新规划整个路径。

RRT算法的优势体现在其对高维空间的良好适应性和较低的内存需求上。算法实现相对简单，能够处理复杂的约束条件，且易于扩展到动态环境。但RRT的劣势在于其概率性质导致结果不可重复，生成的路径通常不是最优的，且收敛速度可能较慢。

## 最优路径保证问题

A算法在使用可容许启发式函数的前提下，能够数学上保证找到最优路径。这种确定性是A*算法的核心优势之一。

相比之下，基本的RRT算法无法保证找到最优路径。它是概率完备的，意味着随着采样点数量增加，找到解的概率趋向于1，但找到的解通常不是最优的。为了解决这个问题，研究者开发了RRT*算法，通过重新布线机制逐步优化路径，在渐近意义下能够收敛到最优解。

## RRT参数对结果的影响

步长参数直接影响算法的探索能力和路径质量。较大的步长能够加快搜索速度，但可能导致路径粗糙且难以通过狭窄通道。较小的步长虽然能生成更平滑的路径，但会显著增加计算时间，在开阔区域的探索效率较低。

目标概率参数控制算法向目标偏置的程度。适当的目标偏置能够引导搜索朝目标方向进行，提高收敛速度。然而，过高的目标概率可能导致算法陷入局部最优，特别是在目标附近存在障碍物时。过低的目标概率则可能导致搜索过于随机，收敛速度缓慢。

迭代次数决定了算法的搜索深度和找到解的概率。更多的迭代次数通常能提高找到解的概率并改善路径质量，但同时会增加计算成本。在实际应用中，需要在解的质量和计算效率之间找到平衡点。

总的来说，A*算法适合小规模、障碍物相对简单且对最优性要求较高的场景，而RRT算法更适合大规模、高维度或动态变化的复杂环境。实际选择时需要根据具体应用需求、计算资源限制和环境特点来决定使用哪种算法。



A*的变体，简单了解了以下几种

1.启发式函数改进

2.内存优化变体

3.多目标和动态环境适应      D*（动态化）

4.并行化改进

5.搜索策略优化   加权A*    

6.特定领域优化

7.近似算法



需要去了解哪些？





### 地图环境研究

**渐进式地图探索**

- 研究在逐渐展开的地图环境中算法的表现                 
- 地图信息不完整时的路径规划策略    dijkstra？
- **渐进式地图如何去入手研究？**

### 多智能体系统

**单agent vs 多agent场景**

- 针对三个目标点的攻击任务完成
- 多智能体协调与交互机制
- 不同agent的能力假设和交互模型
- **不知道如何入手分析**

### 学习与优化

**强化学习整合**

- 将传统路径规划与强化学习结合（能处理动态环境）
- agent的持续学习和适应能力

### 研究方法

- 理论原理掌握
- 实验验证效果
- 感知能力的优劣势分析

这个研究方向涉及传统路径规划算法、多智能体系统和强化学习的交叉领域，是一个很有深度的研究课题。你们是准备从哪个方面开始深入研究？