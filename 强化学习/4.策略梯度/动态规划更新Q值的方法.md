动态规划方法使用完整的环境模型，通过贝尔曼方程进行Q值更新。具体来说：

Q(s,a) = R(s,a) + γ ∑ P(s'|s,a) max Q(s',a')


状态动作对的Q值更新策略：

组成部分：


该状态的奖励R

在该状态采取动作a对应的下一状态s'对应的最大Q值

而下一个状态s'有多个，对每个状态概率加权的最大Q值求和

再乘学习率γ

二者相加就是Q值更新策略

这里需要知道状态转移概率P(s'|s,a)和奖励函数R(s,a)。动态规划会系统性地遍历所有状态-动作对，使用当前的Q值估计来计算新的Q值，直到收敛



与时序差分方法的主要区别：

计算方式差异：动态规划使用期望值计算（对所有可能的下一状态求期望），而时序差分使用实际观察到的样本进行增量更新。

模型依赖性：动态规划需要完整的环境模型（转移概率和奖励函数），而时序差分是无模型的，只需要与环境交互获得的经验。

更新策略：动态规划通常是同步更新所有状态的Q值，而时序差分是在线增量更新，每次交互后立即更新相关的Q值。

收敛保证：在有限状态空间中，动态规划在有限步内收敛到最优解；时序差分在满足一定条件下（如所有状态-动作对被无限次访问）也能收敛到最优解。

实用性：时序差分更适合实际应用，因为很多现实问题中我们无法获得完整的环境模型，而动态规划更多用于理论分析和已知模型的小规模问题。

这两种方法都基于贝尔曼最优性原理，但在实现路径上体现了"基于模型"与"无模型"学习的根本区别。
