### V(s) - 状态价值函数

* 表示在状态s下，遵循某个策略π能够获得的**期望累积回报**
* 只依赖于状态s，不考虑具体采取什么动作
* 回答的问题是："处在这个状态有多好？"
* 数学表达：V^π(s) = E[G\_t | S\_t = s]

Q(s,a) - 动作价值函数

* 表示在状态s下采取动作a，然后遵循策略π能够获得的**期望累积回报**
* 同时依赖于状态s和动作a
* 回答的问题是："在这个状态下采取这个动作有多好？"
* 数学表达：Q^π(s,a) = E[G\_t | S\_t = s, A\_t = a]

**两者关系**
它们之间存在重要的数学关系：
V^π(s) = Σ\_a π(a|s) × Q^π(s,a)

这意味着状态价值等于所有可能动作的Q值按照策略概率加权的期望值。

**实际应用差异**

* V(s)更适合**策略评估**，评判某个策略的好坏
* Q(s,a)更适合**策略改进**，因为可以直接比较不同动作的价值来选择最优动作

在算法实现中，Q-learning等方法通常直接学习Q函数，因为它可以直接用于动作选择，而学习V函数还需要额外的模型信息才能选择动作。
