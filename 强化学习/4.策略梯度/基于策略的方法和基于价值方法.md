# 基于价值的方法 vs 基于策略的方法

## 1. 核心概念对比

### 1.1 基本思路差异

**基于价值的方法（Value-based）**：

```
学习路径：环境 → 价值函数 → 策略
核心思想：先评估状态/动作的好坏，再据此选择动作
从环境学习动作价值，通过每个状态动作的价值再形成一套策略
环境交互 → 学习价值函数 → 导出策略
```

**基于策略的方法（Policy-based）**：

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">∇</mi><msub><mover accent="true"><mi>R</mi><mo>ˉ</mo></mover><mi>θ</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac><munderover><mo>∑</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><munderover><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>n</mi></msub></munderover><mi>R</mi><mrow><mo fence="true">(</mo><msup><mi>τ</mi><mi>n</mi></msup><mo fence="true">)</mo></mrow><mi mathvariant="normal">∇</mi><mi>log</mi><mo>⁡</mo><msub><mi>p</mi><mi>θ</mi></msub><mrow><mo fence="true">(</mo><msubsup><mi>a</mi><mi>t</mi><mi>n</mi></msubsup><mi mathvariant="normal">∣</mi><msubsup><mi>s</mi><mi>t</mi><mi>n</mi></msubsup><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">\\nabla \\bar{R}\_{\\theta} = \\frac{1}{N}\\sum\_{n = 1}^{N}\\sum\_{t = 1}^{T\_{n}}R\\left(\\tau^{n}\\right)\\nabla \\log p\_{\\theta}\\left(a\_{t}^{n}|s\_{t}^{n}\\right)</annotation></semantics></math>

双层求和

外层求和是N次不同的实验

内层求和是单次实验

R是单步的即时奖励

logpθ(aⁿₜ|sⁿₜ)是单次动作的概率对数

取梯度代表θ参数改进的方向

一共t个状态下采取的动作，在R奖励评估下，这些动作对θ参数调整的影响

比如该次实验的R奖励是正的，那么该次实验中所有的状态-动作对的概率都会提升

提升的方法——改变θ参数的数值

也就是说之后的θ会增加在这些状态时采取相应动作的概率

```
学习路径：环境 → 策略
核心思想：直接学习从状态到动作的映射
不学习动作的价值，通过策略梯度上升公式，通过即时奖励直接更新策略
策略梯度更新的底层就是增大有奖励动作的执行概率，减小有惩罚动作的执行概率，落实在概率上
```

### 1.2 数学表示

**价值方法**：

```python
# 学习价值函数
V^π(s) = E[G_t | S_t = s]
Q^π(s,a) = E[G_t | S_t = s, A_t = a]

# 导出策略
π(s) = argmax_a Q(s,a)  # 确定性策略
π(a|s) = softmax(Q(s,a)/τ)  # 随机策略（Boltzmann）
```

**策略方法**：

```python
# 直接参数化策略
π_θ(a|s) = P(A_t = a | S_t = s, θ)

# 优化目标
J(θ) = E[∑ γ^t R_t]
θ* = argmax_θ J(θ)
```

## 2. 详细对比分析

### 2.1 学习目标

| 维度               | 基于价值                | 基于策略         |
| ------------------ | ----------------------- | ---------------- |
| **学习对象** | 价值函数 V(s) 或 Q(s,a) | 策略函数 π(s,a) |
| **优化目标** | 最小化价值预测误差      | 最大化期望回报   |
| **输出**     | 状态/动作价值           | 动作概率分布     |

```python
# 价值方法的损失函数
def value_loss(predicted_value, target_value):
    return (predicted_value - target_value) ** 2

# 策略方法的目标函数
def policy_objective(log_prob, advantage):
    return log_prob * advantage  # 策略梯度
```

### 2.2 策略表示方式

**价值方法的策略**：

```python
class ValueBasedAgent:
    def __init__(self):
        self.Q = {}  # Q表或Q网络
  
    def select_action(self, state):
        # 隐式策略：贪心选择
        return np.argmax(self.Q[state])
  
    def epsilon_greedy(self, state, epsilon=0.1):
        # 显式探索策略
        if np.random.random() < epsilon:
            return np.random.choice(self.action_space)
        return np.argmax(self.Q[state])
```

**策略方法的策略**：

```python
class PolicyBasedAgent:
    def __init__(self):
        self.policy_net = PolicyNetwork()
  
    def select_action(self, state):
        # 显式策略：概率采样
        action_probs = self.policy_net(state)
        return np.random.choice(len(action_probs), p=action_probs)
  
    def get_action_prob(self, state, action):
        # 可以直接获取动作概率
        return self.policy_net(state)[action]
```

### 2.3 动作空间适应性

**离散动作空间**：

```python
# 价值方法：自然适合
class DQN:
    def __init__(self, state_dim, action_dim):
        # 输出每个动作的Q值
        self.q_net = nn.Linear(state_dim, action_dim)
  
    def forward(self, state):
        return self.q_net(state)  # [Q(s,a1), Q(s,a2), ...]

# 策略方法：同样适合
class PolicyNet:
    def __init__(self, state_dim, action_dim):
        self.net = nn.Sequential(
            nn.Linear(state_dim, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)  # 输出概率分布
        )
```

**连续动作空间**：

```python
# 价值方法：需要特殊处理
class DDPG_Critic:
    def __init__(self, state_dim, action_dim):
        # 需要同时输入状态和动作
        self.q_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, 64),
            nn.ReLU(),
            nn.Linear(64, 1)  # 输出单个Q值
        )

# 策略方法：自然适合
class ContinuousPolicy:
    def __init__(self, state_dim, action_dim):
        self.mean_net = nn.Linear(state_dim, action_dim)
        self.std_net = nn.Linear(state_dim, action_dim)
  
    def forward(self, state):
        mean = self.mean_net(state)
        std = torch.exp(self.std_net(state))
        return Normal(mean, std)  # 高斯分布
```

## 3. 算法实例对比

### 3.1 经典算法

**价值方法算法**：

- **Q-learning**: 学习Q函数，off-policy
- **SARSA**: 学习Q函数，on-policy
- **DQN**: 深度Q网络
- **Double DQN**: 解决过估计问题

**策略方法算法**：

- **REINFORCE**: 基础策略梯度
- **Actor-Critic**: 结合价值估计
- **PPO**: 近端策略优化
- **TRPO**: 信任域策略优化

### 3.2 具体实现对比

**Q-learning实现**：

```python
class QLearning:
    def __init__(self, state_dim, action_dim, lr=0.1, gamma=0.9):
        self.Q = np.zeros((state_dim, action_dim))
        self.lr = lr
        self.gamma = gamma
  
    def update(self, state, action, reward, next_state, done):
        # TD更新
        target = reward
        if not done:
            target += self.gamma * np.max(self.Q[next_state])
  
        td_error = target - self.Q[state, action]
        self.Q[state, action] += self.lr * td_error
  
    def get_action(self, state, epsilon=0.1):
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        return np.argmax(self.Q[state])
```

**REINFORCE实现**：

```python
class REINFORCE:
    def __init__(self, state_dim, action_dim, lr=0.01, gamma=0.9):
        self.policy = PolicyNetwork(state_dim, action_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)
        self.gamma = gamma
  
    def get_action(self, state):
        probs = self.policy(torch.FloatTensor(state))
        dist = Categorical(probs)
        action = dist.sample()
        return action.item(), dist.log_prob(action)
  
    def update(self, trajectory):
        policy_loss = []
        returns = self.calculate_returns(trajectory)
  
        for (log_prob, _), G in zip(trajectory, returns):
            policy_loss.append(-log_prob * G)
  
        self.optimizer.zero_grad()
        loss = torch.stack(policy_loss).sum()
        loss.backward()
        self.optimizer.step()
```

## 4. 优缺点对比

### 4.1 基于价值的方法

**优点**：

```python
✓ 样本效率高（特别是off-policy方法）
✓ 学习稳定，收敛性好
✓ 可以重用历史经验（经验回放）
✓ 理论基础扎实
```

**缺点**：

```python
✗ 难处理连续动作空间
✗ 策略改进可能不平滑
✗ 在高维动作空间中计算复杂
✗ 确定性策略可能陷入局部最优
```

### 4.2 基于策略的方法

**优点**：

```python
✓ 自然处理连续动作空间
✓ 可以学习随机策略
✓ 策略改进平滑
✓ 适合部分可观测环境
✓ 可以融入先验知识
```

**缺点**：

```python
✗ 样本效率相对较低
✗ 训练方差大，不稳定
✗ 容易收敛到局部最优
✗ 需要完整轨迹进行更新
```

## 5. 选择指南

### 5.1 何时选择价值方法

```python
# 适合场景
scenarios = {
    "离散动作空间": "如游戏、导航",
    "样本获取困难": "需要高样本效率",
    "确定性环境": "最优策略是确定性的",
    "在线学习": "需要快速适应"
}

# 典型应用
applications = [
    "Atari游戏",
    "棋类游戏",
    "离散控制问题",
    "资源分配"
]
```

### 5.2 何时选择策略方法

```python
# 适合场景
scenarios = {
    "连续动作空间": "如机器人控制",
    "需要随机策略": "如多智能体、部分可观测",
    "复杂策略结构": "需要特定的策略形式",
    "探索重要": "环境奖励稀疏"
}

# 典型应用
applications = [
    "机器人控制",
    "自动驾驶",
    "金融交易",
    "自然语言生成"
]
```

## 6. 混合方法：Actor-Critic

结合两种方法的优点：

```python
class ActorCritic:
    def __init__(self, state_dim, action_dim):
        # Actor: 策略网络
        self.actor = PolicyNetwork(state_dim, action_dim)
        # Critic: 价值网络
        self.critic = ValueNetwork(state_dim)
  
    def update(self, state, action, reward, next_state, done):
        # Critic更新（价值方法）
        value = self.critic(state)
        next_value = self.critic(next_state) if not done else 0
        td_target = reward + self.gamma * next_value
        value_loss = (td_target - value) ** 2
  
        # Actor更新（策略方法）
        advantage = td_target - value
        log_prob = self.actor.log_prob(state, action)
        policy_loss = -log_prob * advantage.detach()
  
        # 联合优化
        total_loss = policy_loss + value_loss
        total_loss.backward()
```

这种混合方法试图结合两种方法的优势，在实践中往往表现更好。
