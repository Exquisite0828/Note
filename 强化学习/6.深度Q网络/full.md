# 第6章 深度Q网络

传统的强化学习算法会使用表格的形式存储状态价值函数  $V(s)$  或动作价值函数  $Q(s,a)$  ，但是这样的方法存在很大的局限性。例如，现实中的强化学习任务所面临的状态空间往往是连续的，存在无穷多个状态，在这种情况下，就不能再使用表格对价值函数进行存储。价值函数近似利用函数直接拟合状态价值函数或动作价值函数，降低了对存储空间的要求，有效地解决了这个问题。

为了在连续的状态和动作空间中计算值函数  $Q_{\pi}(s,a)$  ，我们可以用一个函数  $Q_{\phi}(s,a)$  来表示近似计算，称为价值函数近似（value function approximation）。

$$
Q_{\phi}(s,a)\approx Q_{\pi}(s,a) \tag{6.1}
$$

其中，  $s,a$  分别是状态  $s$  和动作  $a$  的向量表示，函数  $Q_{\phi}(s,a)$  通常是一个参数为  $\phi$  的函数，比如神经网络，其输出为一个实数，称为Q网络（Q- network）。

深度Q网络（deepQ- network，DQN）是指基于深度学习的Q学习算法，主要结合了价值函数近似与神经网络技术，并采用目标网络和经历回放的方法进行网络的训练。在Q学习中，我们使用表格来存储每个状态  $s$  下采取动作  $a$  获得的奖励，即状态- 动作值函数  $Q(s,a)$  。然而，这种方法在状态量巨大甚至是连续的任务中，会遇到维度灾难问题，往往是不可行的。因此，深度Q网络采用了价值函数近似的表示方法。

# 6.1 状态价值函数

深度Q网络是基于价值的算法，在基于价值的算法里面，我们学习的不是策略，而是评论员(critic)。评论员的任务是评价现在的动作有多好或有多不好。假设有一个演员，其要学习一个策略来得到尽量高的回报。评论员就是评价演员的策略  $\pi$  好还是不好，即策略评估。例如，有一种评论员称为状态价值函数 $V_{\pi}$  。状态价值函数是指，假设演员的策略是  $\pi$  ，用  $\pi$  与环境交互，假设  $\pi$  看到了某一个状态  $s$  ，例如在玩雅达利游戏，状态  $s$  是某一个画面，  $\pi$  看到某一个画面，接下来一直到游戏结束，期望的累积奖励有多大。如图6.1a所示，  $V_{\pi}$  是一个函数，输入一个状态，它会输出一个标量。这个标量代表演员的策略  $\pi$  看到状态  $s$  的时候，预期到游戏结束的时候，它可以获得多大的奖励。例如，假设我们在玩太空侵略者，图6.1b所示的状态  $s$  ，这个游戏画面，  $V_{\pi}(s)$  也许会很大，因为这时还有很多的怪兽可以击杀，所以我们会得到很高的分数。一直到游戏结束的时候，我们仍然有很多的分数可以获得。图6.1c所示的情况我们得到的 $V_{\pi}(s)$  可能就很小，因为剩下的怪兽也不多，并且红色的防护罩已经消失了，所以我们可能很快就会“死掉”。因此接下来得到预期的奖励，就不会太大。

![](images/5048e25f7ad8184db1f96e250fc17be14866ab528120170f8c804ea67deaf25e.jpg)  
图6.1 玩太空侵略者

这里需要强调，评论员无法凭空评价一个状态的好坏，它所评价的是在给定某一个状态的时候，如果接下来交互的演员的策略是  $\pi$  ，我们会得到多少奖励，这个奖励就是我们评价得出的值。因为就算是同样的状态，接下来的  $\pi$  不一样，得到的奖励也是不一样的。例如，在左边的情况下，假设是一个正常的  $\pi$  它可以击杀很多怪兽；假设它是一个很弱的  $\pi$  ，它就站在原地不动，马上就被射死了，我们得到的  $V_{\pi}(s)$

还是很小。所以评论员的输出值取决于状态和演员。评论员其实都要绑定一个演员，它是在衡量某一个演员的好坏，而不是衡量一个状态的好坏。这里要强调一下，评论员的输出是与演员有关的，状态的价值其实取决于演员，当演员改变的时候，状态价值函数的输出其实也是会跟着改变的。

怎么衡量状态价值函数  $V_{\pi}(s)$  呢？有两种不同的方法：基于蒙特卡洛的方法和基于时序差分的方法。基于蒙特卡洛的方法就是让演员与环境交互，我们要看演员好不好，就让演员与环境交互，让评论员评价。评论员就统计，演员如果看到状态  $s_{a}$ ，接下来的累积奖励有多大；如果它看到状态  $s_{b}$ ，接下来的累积奖励有多大。但是实际上，我们不可能看到所有的状态。如果我们在玩雅达利游戏，状态是图像，那么无法看到所有的状态。所以实际上  $V_{\pi}(s)$  是一个网络。对一个网络来说，就算输入状态是从来都没有看过的，它也可以想办法估测一个值。怎么训练这个网络呢？如图6.2所示，如果在状态  $s_{a}$ ，接下来的累积奖励就是  $G_{a}$  。也就是对这个价值函数，如果输入是状态  $s_{a}$ ，正确的输出应该是  $G_{a}$  。如果输入状态是  $s_{b}$ ，正确的输出应该是  $G_{b}$  。所以在训练的时候，它就是一个回归问题（regression problem）。网络的输出就是一个值，我们希望在输入  $s_{a}$  的时候，输出的值与  $G_{a}$  越接近越好；输入  $s_{b}$  的时候，输出的值与  $G_{b}$  越接近越好。接下来继续训练网络，这是基于蒙特卡洛的方法。

![](images/daba41a8417669560dd753d5bb456f5f99809a69fe04285c14809d853cd0f7ae.jpg)  
图6.2 基于蒙特卡洛的方法

第二个方法是时序差分的方法，即基于时序差分的方法。在基于蒙特卡洛的方法中，每次我们都要计算累积奖励，也就是从某一个状态  $s_{a}$  一直到游戏结束的时候，得到的所有奖励的总和。如果我们要使用基于蒙特卡洛的方法，我们必须至少玩到游戏结束。但有些游戏时间非常长，我们要玩到游戏结束才能够更新网络，这花的时间太多了，因此我们会采用基于时序差分的方法。基于时序差分的方法不需要玩到游戏结束，只需要在游戏的某一个状态  $s_{t}$  的时候，采取动作  $a_{t}$  得到奖励  $r_{t}$ ，接下来进入状态  $s_{t + 1}$ ，就可以使用时序差分的方法。我们可以通过式(6.2)来使用时序差分的方法。

$$
V_{\pi}(s_{t}) = V_{\pi}(s_{t + 1}) + r_{t} \tag{6.2}
$$

假设我们现在用的是某一个策略  $\pi$ ，在状态  $s_{t}$  时，它会采取动作  $a_{t}$ ，得到奖励  $r_{t}$ ，接下来进入  $s_{t + 1}$  。状态  $s_{t + 1}$  的值与状态  $s_{t}$  的值，它们的中间差了一项  $r_{t}$ ，这是因为我们把  $s_{t + 1}$  的值加上得到的奖励  $r_{t}$  就可以得到  $s_{t}$  的值。有了式(6.2)，在训练的时候，我们并不是直接估测  $V_{\pi}$ ，而是希望得到的结果  $V_{\pi}$  可以满足式(6.2)。我们是这样训练的，如图6.3所示，我们把  $s_{t}$  输入网络，因为把  $s_{t}$  输入网络会得到  $V_{\pi}(s_{t})$ ，把  $s_{t + 1}$  输入网络会得到  $V_{\pi}(s_{t + 1})$ ， $V_{\pi}(s_{t})$  减  $V_{\pi}(s_{t + 1})$  的值应该是  $r_{t}$ 。我们希望它们相减的损失与  $r_{t}$  接近，训练下去，更新  $V_{\pi}$  的参数，我们就可以把  $V_{\pi}$  函数学习出来。

蒙特卡洛方法与时序差分方法有什么差别呢？如图6.4所示，蒙特卡洛方法最大的问题就是方差很大。因为我们在玩游戏的时候，游戏本身是有随机性的，所以我们可以把  $G_{a}$  看成一个随机变量。因为我们每次到  $s_{a}$  的时候，最后得到的  $G_{a}$  其实是不一样的。我们看到同样的状态  $s_{a}$ ，最后到游戏结束的时候，因为游戏本身是有随机性的，玩游戏的模型可能也有随机性，所以我们每次得到的  $G_{a}$  是不一样的，每一次得到的  $G_{a}$  的差别其实会很大。为什么会很大呢？因为  $G_{a}$  是很多个不同的步骤的奖励的和。假设我们每一个步骤都会得到一个奖励， $G_{a}$  是从状态  $s_{a}$  开始一直到游戏结束，每一个步骤的奖励的和。

![](images/b74ce3402ab4c7da516abd342889b8e4e709c9d2fbce20d05810a1eb021824ad.jpg)  
图6.3 基于时序差分的方法

![](images/0602e6cf70d743854440ea8f7ce988d468d2d012d9fe3b13c74d3c863f04fa97.jpg)  
图6.4 蒙特卡洛方法的问题

通过式(6.3)，我们知道  $G_{a}$  的方差相较于某一个状态的奖励，它是比较大的。

$$
\mathrm{Var}[kX] = k^2\mathrm{Var}[X] \tag{6.3}
$$

其中，Var是指方差（variance）。

如果用时序差分的方法，我们要去最小化

$$
V_{\pi}(s_t)\longleftrightarrow r + V_{\pi}(s_{t + 1}) \tag{6.4}
$$

其中，  $r$  具有随机性。因为即使我们在  $s_t$  采取同一个动作，得到的奖励也不一定是一样的，所以  $r$  是一个随机变量。但  $r$  的方差比  $G_{a}$  要小，因为  $G_{a}$  是很多  $r$  的加和，时序差分只是某一个  $r$  而已。  $G_{a}$  的方差会比较大，  $r$  的方差会比较小。但是这里我们会遇到的一个问题是  $V_{\pi}$  的估计不一定准确。假设  $V_{\pi}$  的估计不准确，我们使用式(6.4)学习出来的结果也会是不准确的。所以蒙特卡洛方法与时序差分方法各有优劣。其实时序差分方法是比较常用的，蒙特卡洛方法其实是比较少用的。

图6.5所示为时序差分方法与蒙特卡洛方法的差别。假设有某一个评论员，它去观察某一个策略  $\pi$  与环境交互8个回合的结果。有一个策略  $\pi$  与环境交互了8次，得到了8次玩游戏的结果。接下来这个评论员去估测状态的值。

我们先计算  $s_b$  的值。状态  $s_b$  在8场游戏里都存在，其中有6场得到奖励1，有2场得到奖励0。所以如果我们要计算期望值，只算智能体看到状态  $s_b$  以后得到的奖励。智能体一直玩到游戏结束的时候得到的累积奖励期望值是  $3 / 4$  ，计算过程为

$$
\frac{6\times 1 + 2\times 0}{8} = \frac{6}{8} = \frac{3}{4} \tag{6.5}
$$

但  $s_{a}$  期望的奖励到底应该是多少呢？这里其实有两个可能的答案：0和  $3 / 4$  。为什么有两个可能的答案呢？这取决于我们用蒙特卡洛方法还是时序差分方法。用蒙特卡洛方法与用时序差分方法算出来的结果是不一样的。

假如我们用蒙特卡洛方法，  $s_{a}$  就出现一次，看到状态  $s_{a}$  ，接下来累积奖励就是0，所以  $s_{a}$  期望奖励就是0。但时序差分方法在计算的时候，需要更新

$$
V_{\pi}(s_{a}) = V_{\pi}(s_{b}) + r \tag{6.6}
$$

因为我们在状态  $s_{a}$  得到奖励  $r = 0$  以后，进入状态  $s_{b}$  ，所以状态  $s_{a}$  的奖励等于状态  $s_{b}$  的奖励加上从状态  $s_{a}$  进入状态  $s_{b}$  的时候可能得到的奖励  $r$  。而得到的奖励  $r$  的值是0，  $s_{b}$  期望奖励是  $3 / 4$  ，那么  $s_{a}$  的奖励应该是  $3 / 4$  。

用蒙特卡洛方法与时序差分方法估出来的结果很有可能是不一样的。就算评论员观察到一样的训练数据，它最后估出来的结果也不一定是一样的。为什么会这样呢？哪一个结果比较对呢？其实都对。因为在第一个轨迹中，  $s_{a}$  得到奖励0以后，再进入  $s_{b}$  也得到奖励0。这里有两个可能。

(1)  $s_{a}$  是一个标志性的状态，只要看到  $s_{a}$  以后，  $s_{b}$  就不会获得奖励，  $s_{a}$  可能影响了  $s_{b}$  。如果是使用蒙特卡洛方法，它会把  $s_{a}$  影响  $s_{b}$  这件事考虑进去。所以看到  $s_{a}$  以后，接下来  $s_{b}$  就得不到奖励，  $s_{b}$  期望的奖励是0。

(2）看到  $s_{a}$  以后，  $s_{b}$  的奖励是0这件事只是巧合，并不是  $s_{a}$  造成的，而是因为  $s_{b}$  有时候就是会得到奖励0，这只是单纯“运气”的问题。其实平常  $s_{b}$  会得到的奖励期望值是  $3 / 4$  ，与  $s_{a}$  是完全没有关系的。所以假设  $s_{a}$  之后会进入  $s_{b}$  ，得到的奖励按照时序差分方法来算应该是  $3 / 4$  。

不同的方法考虑了不同的假设，所以运算结果不同。

# 评论员部分有以下8个回合

![](images/b32fbe8fa558743aad9a43a0fd0d8959d2289c5ecdab1895fef0607e50633eda.jpg)  
图6.5 时序差分方法与蒙特卡洛方法的差别

# 6.2 动作价值函数

还有另外一种评论员称为Q函数，它又被称为动作价值函数。状态价值函数的输入是一个状态，它根据状态计算出这个状态以后的期望的累积奖励（expected accumulated reward）是多少。动作价值函数的输入是一个状态- 动作对，其指在某一个状态采取某一个动作，假设我们都使用策略  $\pi$  ，得到的累积奖励的期望值有多大。

Q函数有一个需要注意的问题是，策略  $\pi$  在看到状态  $s$  的时候，它采取的动作不一定是  $a$  。Q函数假设在状态  $s$  强制采取动作  $a$  ，而不管我们现在考虑的策略  $\pi$  会不会采取动作  $a$  ，这并不重要。在状态  $s$  强制采取动作  $a$  。接下来都用策略  $\pi$  继续玩下去，就只有在状态  $s$  ，我们才强制一定要采取动作  $a$  ，接下来就进入自动模式，让策略  $\pi$  继续玩下去，得到的期望奖励才是  $Q_{\pi}(s,a)$  。

Q函数有两种写法：

（1）如图6.6a所示，输人是状态与动作，输出就是一个标量。这种Q函数既适用于连续动作（动作是无法穷举的)，又适用于离散动作。

(2）如图6.6b所示，输入是一个状态，输出就是多个值。这种Q函数只适用于离散动作。假设动作是离散的，比如动作就只有3个可能：往左、往右或是开火。Q函数输出的3个值就分别代表  $a$  是往左的时候的Q值，  $a$  是往右的时候的Q值，还有  $a$  是开火的时候的Q值。

如果我们去估计Q函数，看到的结果可能如图6.7所示。假设我们有3个动作：原地不动、向上、向下。假设在第一个状态，不管采取哪个动作，最后到游戏结束的时候，得到的期望奖励都差不多。因为乒乓球在这个地方时，就算我们向下，接下来我们应该还可以接到乒乓球，所以不管采取哪个动作，都相差

![](images/0176ab297d31879f03f5317b4319844c6c58548cd23d0b8651db22d706997b72.jpg)  
图6.6 Q函数

不了太多。假设在第二个状态，乒乓球已经反弹到很接近边缘的地方，这个时候我们采取向上的动作，才能接到乒乓球，才能得到正的奖励。如果我们站在原地不动或向下，接下来都会错过这个乒乓球，得到的奖励就会是负的。假设在第三个状态，乒乓球离我们的球拍很近了，所以就要采取向上的动作。假设在第四个状态，乒乓球被反弹回去，这时候采取哪个动作都差不多。这是动作价值函数的例子。

![](images/3574ba030e6b066cb6475b929951c3dd6cb3405bf03b137169eb282476cdc86f.jpg)  
图6.7 乒乓球例子[1]

虽然我们学习的Q函数只能用来评估某一个策略  $\pi$  的好坏，但只要有了Q函数，我们就可以进行强化学习，就可以决定要采取哪一个动作，就可以进行策略改进。如图6.8所示，假设我们有一个初始的演员，也许一开始很差，随机的也没有关系。初始的演员称为  $\pi$  ，  $\pi$  与环境交互，会收集数据。接下来我们学习策略  $\pi$  的Q值，去衡量一下  $\pi$  在某一个状态强制采取某一个动作，接下来会得到的期望奖励，用时序差分方法或蒙特卡洛方法都是可以的。我们学习出一个Q函数以后，就可以找到一个新的策略  $\pi^{\prime}$  ，策略  $\pi^{\prime}$  会比原来的策略  $\pi$  要好（稍后会定义什么是好）。所以假设我们有一个Q函数和某一个策略  $\pi$  ，根据策略  $\pi$  学习出策略  $\pi$  的Q函数，接下来可以找到一个新的策略  $\pi^{\prime}$  ，它会比  $\pi$  要好。我们用  $\pi^{\prime}$  取代  $\pi$  再去学习它的Q函数，得到新的Q函数以后，再去寻找一个更好的策略。这样一直循环下去，策略就会越来越好。

首先要定义的是什么是好，  $\pi^{\prime}$  一定会比  $\pi$  要好，什么是好呢？这里的好是指，对所有可能的状态  $s$  而言，  $V_{\pi^{\prime}}(s)\geqslant V_{\pi}(s)$  。也就是我们到同一个状态  $s$  的时候，如果用  $\pi$  继续与环境交互，我们得到的奖励一定会小于等于用  $\pi^{\prime}$  与环境交互得到的奖励。所以不管在哪一个状态，我们用  $\pi^{\prime}$  与环境交互，得到的期望奖励一定会比较大。所以  $\pi^{\prime}$  是比  $\pi$  要好的策略。

有了Q函数以后，我们把根据式(6.7)决定动作的策略称为  $\pi^{\prime}$

$$
\pi^{\prime}(s) = \underset {a}{\arg \max}Q_{\pi}(s,a) \tag{6.7}
$$

$\pi^{\prime}$  一定比  $\pi$  好。假设我们已经学习出  $\pi$  的Q函数，在某一个状态  $s$  ，把所有可能的动作  $a$  一一代入Q函数，看看哪一个  $a$  可以让Q函数的值最大，这个动作就是  $\pi^{\prime}$  会采取的动作。

这里要注意，给定状态  $s$  和策略  $\pi$  并不一定会采取动作  $a$  。给定某一个状态  $s$  强制采取动作  $a$  ，用 $\pi$  继续交互得到的期望奖励，这才是Q函数的定义。所以在状态  $s$  下不一定会采取动作  $a$  。用  $\pi^{\prime}$  在状态 $s$  采取动作  $a$  与用  $\pi$  采取的动作不一定是一样的，  $\pi^{\prime}$  所采取的动作会让它得到比较大的奖励。所以  $\pi^{\prime}$  是用Q函数推出来的，没有另外一个网络决定  $\pi^{\prime}$  怎么与环境交互，有Q函数就可以找出  $\pi^{\prime}$  。但是在这里

![](images/4c47fac05cf2a1f15f76a5e3aee70977375e14ef1903c92aa398cfd6bc840b9f.jpg)  
图6.8 使用Q函数进行策略改进

要解决一个arg max操作的问题，如果  $a$  是离散的，如  $a$  只有3个选项，将每个动作都代入Q函数，看哪个动作的Q值最大，这没有问题。但如果  $a$  是连续的，我们要解决arg max操作问题，就不可行。

接下来讲一下为什么用  $Q_{\pi}(s,a)$  决定的  $\pi^{\prime}$  一定会比  $\pi$  好。假设有一个策略  $\pi^{\prime}$  ，它是由  $Q_{\pi}$  决定的。我们要证明对所有的状态  $s$  ，有  $V_{\pi^{\prime}}(s)\geqslant V_{\pi}(s)$  。

怎么证明呢？  $V_{\pi}(s)$  可写为

$$
V_{\pi}(s) = Q_{\pi}(s,\pi (s)) \tag{6.8}
$$

假设在状态  $s$  下按照策略  $\pi$  ，会采取的动作就是  $\pi (s)$  ，我们算出来的  $Q_{\pi}(s,\pi (s))$  会等于  $V_{\pi}(s)$  。一般而言，  $Q_{\pi}(s,\pi (s))$  不一定等于  $V_{\pi}(s)$  ，因为动作不一定是  $\pi (s)$  。但如果这个动作是  $\pi (s)$  ，  $Q_{\pi}(s,\pi (s))$  是等于  $V_{\pi}(s)$  的。

$Q_{\pi}(s,\pi (s))$  还满足如下的关系：

$$
Q_{\pi}(s,\pi (s))\leqslant \max_{a}Q_{\pi}(s,a) \tag{6.9}
$$

因为  $a$  是所有动作里面可以让Q函数取最大值的那个动作，所以  $Q_{\pi}(s,a)$  一定大于等于  $Q_{\pi}(s,\pi (s))$  。 $Q_{\pi}(s,a)$  中的  $a$  就是  $\pi^{\prime}(s)$  ，因为  $\pi^{\prime}(s)$  输出的  $a$  可以让  $Q_{\pi}(s,a)$  最大，所以我们可得

$$
\max_{a}Q_{\pi}(s,a) = Q_{\pi}\left(s,\pi^{\prime}(s)\right) \tag{6.10}
$$

于是

$$
V_{\pi}(s)\leqslant Q_{\pi}\left(s,\pi^{\prime}(s)\right)
$$

也就是在某一个状态，如果我们按照策略  $\pi$  一直执行下去，得到的奖励一定会小于等于在状态  $s$  故意不按照  $\pi$  所指示的方向，而是按照  $\pi^{\prime}$  的方向走一步得到的奖励。但只有第一步是按照  $\pi^{\prime}$  的方向走，只有在状态  $s$  ，才按照  $\pi^{\prime}$  的指示走，接下来我们就按照  $\pi$  的指示走。虽然只有一步之差，但我们得到的奖励一定会比完全按照  $\pi$  得到的奖励要大。

接下来要证

$$
Q_{\pi}\left(s,\pi^{\prime}(s)\right)\leqslant V_{\pi^{\prime}}(s) \tag{6.11}
$$

也就是，只有一步之差，我们会得到比较大的奖励。但假设每步都是不一样的，每步都按照  $\pi^{\prime}$  而不是  $\pi$  ，得到的奖励一定会更大，即  $Q_{\pi}\left(s,\pi^{\prime}(s)\right)$  是指我们在状态  $s_{t}$  采取动作  $a_{t}$  ，得到奖励  $r_{t}$  ，进入状态  $s_{t + 1}$  ，即

$$
Q_{\pi}\left(s,\pi^{\prime}(s)\right) = \mathbb{E}\left[r_{t} + V_{\pi}\left(s_{t + 1}\right)\mid s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right] \tag{6.12}
$$

有的文献上也会说：在状态  $s_{t}$  采取动作  $a_{t}$  ，得到奖励  $r_{t + 1}$  。但意思其实都是一样的。在状态  $s$  按照  $\pi^{\prime}$  采取某一个动作  $a_{t}$  ，得到奖励  $r_{t}$  ，进入状态  $s_{t + 1}$  ，  $V_{\pi}\left(s_{t + 1}\right)$  是状态  $s_{t + 1}$  根据策略  $\pi$  所估出来的值。因为在同样的状态采取同样的动作，我们得到的奖励和会进入的状态不一定一样，所以需要取期望值。

因为  $V_{\pi}(s) \leqslant Q_{\pi}(s, \pi '(s))$ ，也就是  $V_{\pi}(s_{t + 1}) \leqslant Q_{\pi}(s_{t + 1}, \pi '(s_{t + 1}))$ ，所以我们可得

$$
\begin{array}{rl} & {\mathbb{E}\left[r_t + V_\pi (s_{t + 1})\mid s_t = s,a_t = \pi '(s_t)\right]}\\ & {\leqslant \mathbb{E}\left[r_t + Q_\pi (s_{t + 1},\pi '(s_{t + 1}))\mid s_t = s,a_t = \pi '(s_t)\right]} \end{array} \tag{6.13}
$$

因为  $Q_{\pi}(s_{t + 1}, \pi '(s_{t + 1})) = r_{t + 1} + V_{\pi}(s_{t + 2})$ ，所以我们可得

$$
\begin{array}{rl} & {\mathbb{E}\left[r_t + Q_\pi (s_{t + 1},\pi '(s_{t + 1}))\mid s_t = s,a_t = \pi '(s_t)\right]}\\ & {= \mathbb{E}\left[r_t + r_{t + 1} + V_\pi (s_{t + 2})\mid s_t = s,a_t = \pi '(s_t)\right]} \end{array} \tag{6.14}
$$

我们再把式(6.14)代入  $V_{\pi}(s) \leqslant Q_{\pi}(s, \pi '(s))$ ，一直算到回合结束，即

$$
\begin{array}{r l} & {V^{\pi}(s)\leq Q^{\pi}(s,\pi^{\prime}(s))}\\ & {\qquad = E\left[r_{t} + V^{\pi}\left(s_{t + 1}\right)\left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad \leq E\left[r_{t} + Q^{\pi}\left(s_{t + 1},\pi^{\prime}\left(s_{t + 1}\right)\right)\left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad = E\left[r_{t} + r_{t + 1} + V^{\pi}\left(s_{t + 2}\right)\left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad \leq E\left[r_{t} + r_{t + 1} + Q^{\pi}\left(s_{t + 2},\pi^{\prime}\left(s_{t + 2}\right)\right)\left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad = E\left[r_{t} + r_{t + 1} + r_{t + 2} + V^{\pi}\left(s_{t + 3}\right)\left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad \leq \ldots}\\ & {\qquad \leq E\left[r_{t} + r_{t + 1} + r_{t + 2} + \ldots \left|s_{t} = s,a_{t} = \pi^{\prime}\left(s_{t}\right)\right]\right.}\\ & {\qquad = V^{\pi^{\prime}}(s)} \end{array} \tag{6.15}
$$

因此

$$
V_{\pi}(s) \leqslant V_{\pi^{\prime}}(s) \tag{6.16}
$$

我们可以估计某一个策略的  $Q$  函数，接下来就可以找到另外一个策略  $\pi^{\prime}$  比原来的策略  $\pi$  还要更好。

# 6.3 目标网络

接下来讲一些在深度  $Q$  网络里一定会用到的技巧。第一个技巧是目标网络（target network）。我们在学习  $Q$  函数的时候，也会用到时序差分方法的概念。我们现在收集到一个数据，比如在状态  $s_{t}$  采取动作  $a_{t}$  以后，得到奖励  $r_{t}$ ，进入状态  $s_{t + 1}$ 。根据  $Q$  函数，我们可知

$$
Q_{\pi}(s_{t}, a_{t}) = r_{t} + Q_{\pi}(s_{t + 1}, \pi (s_{t + 1})) \tag{6.17}
$$

所以我们在学习的时候，  $Q$  函数输入  $s_{t}$  、  $a_{t}$  得到的值，与输入  $s_{t + 1}$  、  $\pi (s_{t + 1})$  得到的值之间，我们希望它们相差  $r_{t}$ ，这与时序差分方法的概念是一样的。但是实际上这样的输入并不好学习，假设这是一个回归问题，如图6.9所示，  $Q_{\pi}(s_{t}, a_{t})$  是网络的输出，  $r_{t} + Q_{\pi}(s_{t + 1}, \pi (s_{t + 1}))$  是目标，目标是会变动的。当然如果我们要实现这样的训练，其实也没有问题，就是在做反向传播的时候，  $Q_{\pi}$  的参数会被更新，我们会把两个更新的结果加在一起（因为它们是同一个模型  $Q_{\pi}$ ，所以两个更新的结果会加在一起）。但这样会导致训练变得不太稳定，因为假设我们把  $Q_{\pi}(s_{t}, a_{t})$  当作模型的输出，把  $r_{t} + Q_{\pi}(s_{t + 1}, \pi (s_{t + 1}))$  当作目标，我们要去拟合的目标是一直在变动的，这是不太好训练的。

所以我们会把其中一个  $Q$  网络，通常是把图6.9右边的  $Q$  网络固定住。在训练的时候，我们只更新左边的  $Q$  网络的参数，而右边的  $Q$  网络的参数会被固定。因为右边的  $Q$  网络负责产生目标，所以被称为目标网络。因为目标网络是固定的，所以现在得到的目标  $r_{t} + Q_{\pi}(s_{t + 1}, \pi (s_{t + 1}))$  的值也是固定的。我们只调整左边  $Q$  网络的参数，它就变成一个回归问题。我们希望模型输出的值与目标越接近越好，这样会最小化它的均方误差（mean square error）。

在实现的时候，我们会把左边的  $Q$  网络更新多次，再用更新过的  $Q$  网络替换目标网络。但这两个网络不要一起更新，一起更新，结果会很容易不好。一开始这两个网络是一样的。在训练的时候，我们会把

右边的 Q 网络固定住，在做梯度下降的时候，只调整左边 Q 网络的参数。我们可能更新 100 次以后才把参数复制到右边的网络中，把右边网络的参数覆盖，目标值就变了。就好像我们本来在做一个回归问题，训练后把这个回归问题的损失降下去以后，接下来我们把左边网络的参数复制到右边网络，目标值就变了，接下来就要重新训练。

![](images/ee9b756b61435396ad7ee608711b66a4a2b1cf3427a70aa37b8f6485983c8b5d.jpg)  
图6.9 目标网络

如图6.10a所示，我们可以通过猫追老鼠的例子来直观地理解固定目标网络的目的。猫是Q估计，老鼠是Q目标。一开始，猫离老鼠很远，所以我们想让猫追上老鼠。如图6.10b所示，因为Q目标也是与模型参数相关的，所以每次优化后，Q目标也会动。这就导致一个问题，猫和老鼠都在动。如图6.10c所示，猫和老鼠会在优化空间里面到处乱动，这会产生非常奇怪的优化轨迹，使得训练过程十分不稳定。所以我们可以固定Q网络，让老鼠动得不那么频繁，可能让它每5步动一次，猫则是每一步都在动。如果老鼠每5次动一步，猫就有足够的时间来接近老鼠，它们之间的距离会随着优化过程越来越小，最后它们就可以拟合，拟合后就可以得到一个最好的Q网络。

![](images/d04830000587088242cc42e2571c255635a49ad699b2c3eb008af36dbfb36c0f.jpg)  
图6.10 固定目标网络

# 6.4 探索

第二个技巧是探索。当我们使用Q函数的时候，策略完全取决于Q函数。给定某一个状态，我们就穷举所有的动作，采取让Q值最大的动作，即

$$
a = \underset {a}{\arg \max}Q(s,a) \tag{6.18}
$$

使用Q函数来决定动作与使用策略梯度不一样，策略梯度的输出是随机的，它会输出一个动作的分布，我们根据这个动作的分布去采样，所以在策略梯度里面，我们每次采取的动作是不一样的，是有随机性的。像Q函数中，如果我们采取的动作总是固定的，会遇到的问题就是这不是一个好的收集数据的方式。假设我们要估测某一个状态，可以采取动作  $a_1$  、  $a_2$  、  $a_3$  。我们要估测在某一个状态采取某一个动作会得到的Q值，一定要在那一个状态采取过那一个动作，才能估测出它的值。如果没有在那个状态采取过那个动作，我们其实是估测不出它的值的。如果Q函数是一个网络，这个问题可能没有那么严重。但是一般而言，假设Q函数是一个表格，对于没有见过的状态- 动作对，它是估不出值的。如果Q函数是网络，也会有类似的问题，只是没有那么严重，所以假设我们在某一个状态，动作  $a_1$  、  $a_2$  、  $a_3$  都没有采取过，估出来的 $Q(s,a_1)$  、  $Q(s,a_2)$  、  $Q(s,a_3)$  的值可能都是一样的，都是一个初始值，比如0，即

$$
\begin{array}{r}Q(s,a_1) = 0\\ Q(s,a_2) = 0\\ Q(s,a_3) = 0 \end{array} \tag{6.19}
$$

但是如图6.11所示，假设我们在状态  $s$  采取动作  $a_2$  ，它得到的值是正的奖励，  $Q(s,a_2)$  就会比其他动作的Q值要大。在采取动作的时候，谁的Q值最大就采取谁，所以之后永远都只会采取  $a_2$  ，其他的动作就再也不会被采取了，这就会有问题。比如我们去一个餐厅吃饭。假设我们点了某一样菜，比如椒麻鸡，我们觉得还可以。接下来我们每次去就都会点椒麻鸡，再也不点别的菜了，那我们就不知道别的菜是不是会比椒麻鸡好吃，这是一样的问题。

![](images/602abc96e15823da32875520bdaf5dffca87643a81d6efc1432b2c3b57fc342b.jpg)  
图6.11 探索

如果我们没有好的探索，在训练的时候就会遇到这种问题。例如，假设我们用深度Q网络来玩slither.io网页游戏。我们有一条蛇，它在环境里面走来走去，吃到星星，就加分。假设游戏一开始，蛇往上走，然后吃到星星，就可以得到分数，它就知道往上走可以得到奖励。接下来它就再也不会采取往上走以外的动作了，以后就会变成每次游戏一开始，它就往上走，然后游戏结束。所以需要有探索的机制，让智能体知道，虽然根据之前采样的结果，  $a_2$  好像是不错的，但我们至少偶尔也试一下  $a_1$  与  $a_3$  ，说不定它们更好。

这个问题就是探索- 利用窘境（exploration- exploitation dilemma）问题，有两个方法可以解决这个问题：  $\epsilon$  - 贪心和玻尔兹曼探索（Boltzmann exploration）。

$\epsilon$  - 贪心是指我们有  $1 - \epsilon$  的概率会按照Q函数来决定动作，可写为

$$
a = \left\{ \begin{array}{ll}\arg \max_{a}Q(s,a) & \text{,}\text{if} 1 - \epsilon \text{,}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{If}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{.}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{for}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{ if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{f}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{if}\text{} \end{array}\right. \tag{6.20}
$$

通常将  $\epsilon$  设为一个很小的值，  $1 - \epsilon$  可能是0.9，也就是0.9的概率会按照Q函数来决定动作，但是我们有0.1的概率是随机的。通常在实现上  $\epsilon$  会随着时间递减。在最开始的时候，因为不知道哪个动作是

比较好的，所以我们会花比较大的力气探索。接下来，随着训练的次数越来越多，我们已经比较确定哪个动作是比较好的。我们就会减少探索，会把  $\epsilon$  的值变小，主要根据Q函数来决定动作，比较少随机决定动作，这就是  $\epsilon$  - 贪心。

还有一个方法称为玻尔兹曼探索。在玻尔兹曼探索中，我们假设对于任意的  $s$  、  $a$  ，  $Q(s,a)\geqslant 0$  ，因此 $a$  被选中的概率与  $e^{Q(s,a) / T}$  呈正比，即

$$
\pi (a\mid s) = \frac{\mathrm{e}^{Q(s,a) / T}}{\sum_{a'\in A}\mathrm{e}^{Q(s,a') / T}} \tag{6.21}
$$

其中，  $T > 0$  称为温度系数。如果  $T$  很大，所有动作几乎以等概率选择（探索）；如果  $T$  很小，Q值大的动作更容易被选中（利用)；如果  $T$  趋于0，我们就只选择最优动作。

# 6.5 经验回放

第三个技巧是经验回放（experience replay）。如图6.12所示，经验回放会构建一个回放缓冲区(replay buffer)，回放缓冲区又被称为回放内存（replay memory）。回放缓冲区是指现在有某一个策略 $\pi$  与环境交互，它会去收集数据，我们把所有的数据放到一个数据缓冲区（buffer）里面，数据缓冲区里面存储了很多数据。比如数据缓冲区可以存储5万笔数据，每一笔数据就是记得说，我们之前在某一个状态  $s_t$  ，采取某一个动作  $a_t$  ，得到了奖励  $r_t$  ，进入状态  $s_{t + 1}$  。我们用  $\pi$  去与环境交互多次，把收集到的数据放到回放缓冲区里面。回放缓冲区里面的经验可能来自不同的策略，我们每次用  $\pi$  与环境交互的时候，可能只交互10000次，接下来我们就更新  $\pi$  了。但是回放缓冲区里面可以放5万笔数据，所以5万笔数据可能来自不同的策略。回放缓冲区只有在它装满的时候，才会把旧的数据丢掉。所以回放缓冲区里面其实装了很多不同的策略的经验。

![](images/79267cd0a4f2d76a1f7f243f4256694ff804da94a88b6289a41f264e20739932.jpg)  
图6.12 经验回放

如图6.13所示，有了回放缓冲区以后，我们怎么训练Q模型、怎么估Q函数呢？我们会选代地训练Q函数，在每次选代里面，从回放缓冲区中随机挑一个批量（batch）出来，即与一般的网络训练一样，从训练集里面挑一个批量出来。我们采样该批量出来，里面有一些经验，我们根据这些经验去更新Q函数。这与时序差分学习要有一个目标网络是一样的。我们采样一个批量的数据，得到一些经验，再去更新Q函数。

如果某个算法使用了经验回放这个技巧，该算法就变成了一个异策略的算法。因为本来Q是要观察 $\pi$  的经验的，但实际上存储在回放缓冲区里面的这些经验不是通通来自于  $\pi$  ，有些是过去其他的策略所留下来的经验。因为我们不会用某一个  $\pi$  就把整个回放缓冲区装满，拿去测Q函数，  $\pi$  只是采样一些数据放到回放缓冲区里面，接下来就让Q去训练。所以Q在采样的时候，它会采样到过去的一些数据。

这么做有两个好处。第一个好处是，在进行强化学习的时候，往往最花时间的步骤是与环境交互，训练网络反而是比较快的。因为我们用GPU训练其实很快，真正花时间的往往是与环境交互。用回放缓冲区可以减少与环境交互的次数，因为在做训练的时候，经验不需要通通来自于某一个策略。一些过去的策

![](images/4ae135f7d04f7719c9156f9a76bceb6a2638ca41bd4852b007f094986e5004c0.jpg)  
图6.13 使用回放缓冲区训练  $\mathbf{Q}$  函数

略所得到的经验可以放在回放缓冲区里面被使用很多次，被反复的再利用，这样可以比较高效地采样经验。第二个好处是，在训练网络的时候，其实我们希望一个批量里面的数据越多样（diverse）越好。如果批量里面的数据都是同样性质的，我们训练下去，训练结果是容易不好的。如果批量里面都是一样的数据，训练的时候，性能会比较差。我们希望批量里的数据越多样越好。如果回放缓冲区里面的经验通通来自于不同的策略，我们采样到的一个批量里面的数据会是比较多样的。

Q：我们观察  $\pi$  的值，发现里面混杂了一些不是  $\pi$  的经验，这有没有关系？

A：没关系。这并不是因为过去的策略与现在的策略很像，就算过去的策略与现在的策略不是很像，也是没有关系的。主要的原因是我们并不是去采样一个轨迹，我们只采样了一笔经验，所以与是不是异策略这件事是没有关系的。就算是异策略，就算是这些经验不是来自于  $\pi$  ，我们还是可以用这些经验来估测 $Q_{\pi}(s,a)$  0

# 6.6 深度  $\mathbf{Q}$  网络

图6.14所示为一般的深度  $\mathbf{Q}$  网络算法。深度  $\mathbf{Q}$  网络算法是这样的，我们初始化两个网络—  $\mathcal{Q}$  和 $\hat{Q}$  ，  $\hat{Q}$  就等于  $Q$  。一开始目标网络  $\hat{Q}$  与原来的  $\mathbf{Q}$  网络是一样的。在每一个回合中，我们用演员与环境交互，在每一次交互的过程中，都会得到一个状态  $s_t$  ，会采取某一个动作  $a_{t}$  。怎么知道采取哪一个动作  $a_{t}$  呢？我们就根据现在的  $\mathbf{Q}$  函数，但是要有探索的机制。比如我们用玻尔兹曼探索或是  $\epsilon$  - 贪心探索，接下来得到奖励  $r_t$  ，进入状态  $s_{t + 1}$  。所以现在收集到一笔数据  $(s_t,a_t,r_t,s_{t + 1})$  ，我们将其放到回放缓冲区里面。如果回放缓冲区满了，我们就把一些旧的数据丢掉。接下来我们就从回放缓冲区里面去采样数据，采样到的是  $(s_i,a_i,r_i,s_{i + 1})$  。这笔数据与刚放进去的不一定是同一笔，我们可能抽到旧的。要注意的是，我们采样出来不是一笔数据，采样出来的是一个批量的数据，采样一些经验出来。接下来就是计算目标。假设我们采样出一笔数据，根据这笔数据去计算目标。目标要用目标网络  $\hat{Q}$  来计算。目标是：

$$
y = r_{i} + \max_{a}\hat{Q}\left(s_{i + 1},a\right) \tag{6.22}
$$

其中，  $a$  是让  $\hat{Q}$  值最大的动作。因为我们在状态  $s_{i + 1}$  会采取的动作  $a$  就是可以让  $\hat{Q}$  值最大的那一个动作。接下来我们要更新  $\mathbf{Q}$  值，就把它当作一个回归问题。我们希望  $Q(s_{i},a_{i})$  与目标越接近越好。假设已经更新了一定的次数，比如  $C$  次，设  $C = 100$  ，那我们就把  $\hat{Q}$  设成  $Q$  ，这就是深度  $\mathbf{Q}$  网络算法。

Q：深度  $\mathbf{Q}$  网络和  $\mathbf{Q}$  学习有什么不同？

A：整体来说，深度  $\mathbf{Q}$  网络与  $\mathbf{Q}$  学习的目标价值以及价值的更新方式都非常相似。主要的不同点在于：深度  $\mathbf{Q}$  网络将  $\mathbf{Q}$  学习与深度学习结合，用深度网络来近似动作价值函数，而  $\mathbf{Q}$  学习则是采用表格存储；深度  $\mathbf{Q}$  网络采用了经验回放的训练方法，从历史数据中随机采样，而  $\mathbf{Q}$  学习直接采用下一个状态的数据进行学习。

·初始化函数  $Q$  、目标函数  $\hat{Q}$  ，令  $\hat{Q} = Q$  0

·对于每一个回合。

·对于每一个时间步  $t$  0

·对于给定的状态  $s_t$  ，基于  $Q$  (  $\epsilon$  - 贪婪）执行动作  $a_{t}$  0·获得反馈  $r_t$  ，并获得新的状态  $s_{t + 1}$  0·将  $(s_t,a_t,r_t,s_{t + 1})$  存储到缓冲区中。·从缓冲区中采样 (通常以批量形式)  $(s_{i},a_{i},r_{i},s_{i + 1})$  0·目标值是  $y = r_{i} + \max_{a}\hat{Q} (s_{i + 1},a)$  0·更新Q的参数使得  $Q(s_{i},a_{i})$  尽可能接近于  $y$  (回归)。·每C次更新重置  $\hat{Q} = Q$  0

# 6.7 关键词

深度Q网络（deepQ- network，DQN)：基于深度学习的Q学习算法，其结合了价值函数近似（valuefunction approximation）与神经网络技术，并采用目标网络和经验回放等方法进行网络的训练。

状态- 价值函数（state- valuefunction）：其输入为演员某一时刻的状态，输出为一个标量，即当演员在对应的状态时，预期的到过程结束时间段内所能获得的价值。

状态- 价值函数贝尔曼方程（state- value function Bellman equation）：基于状态- 价值函数的贝尔曼方程，它表示在状态  $s_t$  下对累积奖励  $G_t$  的期望。

Q函数（Q- function）：其也被称为动作价值函数（action- valuefunction）。其输入是一个状态- 动作对，即在某一具体的状态采取对应的动作，假设我们都使用某个策略  $\pi$  ，得到的累积奖励的期望值有多大。

目标网络（target network）：其可解决在基于时序差分的网络中，优化目标  $Q_{\pi}(s_t,a_t) = r_t+$ $Q_{\pi}(s_{t + 1},\pi (s_{t + 1}))$  左右两侧会同时变化使得训练过程不稳定，从而增大回归的难度的问题。目标网络选择将右边部分，即  $r_t + Q_{\pi}(s_{t + 1},\pi (s_{t + 1}))$  固定，通过改变左边部分，即  $Q_{\pi}(s_t,a_t)$  中的参数进行回归，这也是深度Q网络应用中比较重要的技巧。

探索（exploration）：我们在使用Q函数的时候，我们的策略完全取决于Q函数，这有可能导致出现对应的动作是固定的某几个数值的情况，而不像策略梯度中的输出是随机的，我们再从随机分布中采样选择动作。这会导致我们继续训练的输入值一样，从而“加重”输出的固定性，导致整个模型的表达能力急剧下降，这就是探索- 利用窘境（exploration- exploitation dilemma）问题。我们可以使用  $\epsilon$  - 贪心和玻尔兹曼探索（Boltzmann exploration）等探索方法进行优化。

经验回放（experience replay）：其会构建一个回放缓冲区（replay buffer）来保存许多经验，每一个经验的形式如下：在某一个状态  $s_t$  ，采取某一个动作  $a_t$  ，得到奖励  $r_t$  ，然后进入状态  $s_{t + 1}$  。我们使用  $\pi$  与环境交互多次，把收集到的经验都存储在回放缓冲区中。当我们的缓冲区“装满”后，就会自动删去最早进入缓冲区的经验。在训练时，对于每一轮迭代都有相对应的批量（batch）（与我们训练普通的网络一样，都是通过采样得到的），然后用这个批量中的经验去更新我们的Q函数。综上，Q函数在采样和训练的时候，会用到过去的经验，所以这里称这个方法为经验回放，其也是深度Q网络应用中比较重要的技巧。

# 6.8 习题

6.8 习题6- 1 为什么在深度 Q 网络中采用价值函数近似的表示方法？6- 2 评论员的输出通常与哪几个值直接相关？6- 3 我们通常怎么衡量状态价值函数  $V_{\pi}(s)$  ？其优势和劣势分别有哪些？

6- 4基于本章正文介绍的基于蒙特卡洛的网络方法，我们怎么训练模型呢？或者我们应该将其看作机器学习中什么类型的问题呢？

6- 5基于本章正文中介绍的基于时序差分的网络方法，具体地，我们应该怎么训练模型呢？

6- 6动作价值函数和状态价值函数的有什么区别和联系？

6- 7请介绍Q函数的两种表示方法

6- 8当得到了Q函数后，我们应当如何找到更好的策略  $\pi^{\prime}$  呢？或者说  $\pi^{\prime}$  的本质是什么？

6- 9解决探索- 利用窘境问题的探索的方法有哪些？

6- 10我们使用经验回放有什么好处？

6- 11在经验回放中我们观察  $\pi$  的价值，发现里面混杂了一些不是  $\pi$  的经验，这会有影响吗？

# 6.9 面试题

6- 1友善的面试官：请问深度Q网络是什么？其两个关键性的技巧分别是什么？

6- 2友善的面试官：那我们继续分析！你刚才提到的深度Q网络中的两个技巧 目标网络和经验回放，其具体作用是什么呢？

6- 3友善的面试官：深度Q网络和Q学习有什么异同点？

6- 4友善的面试官：请问，随机性策略和确定性策略有什么区别吗？

6- 5友善的面试官：请问不打破数据相关性，神经网络的训练效果为什么就不好？

# 参考文献

[1] MNIH V, KAVUKCUOGLI K, SILVER D, et al. Human- level control through deep reinforcement learning[J]. nature, 2015, 518(7540): 529- 533.