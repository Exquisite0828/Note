# 6.1 状态价值函数

评论员critic，评价一个状态的好坏不是凭空判断，是根据在这个状态不同的演员actor策略进行评估，不同的演员策略得到不同的状态价值函数V(S)

衡量状态价值函数V(S)的方法：

1. 蒙特卡洛方法
2. 时序差分方法

## 蒙特卡洛方法

让演员与环境交互，让评论员评价，看到$S_a$接下来累计奖励多大，看到$S_b$接下来累计奖励多大。但理论上，我们不可能看到一个环境的所有状态。每次计算累计奖励时，要等到**游戏结束**，查看所有的奖励，计算每个状态的累计奖励，游戏不结束累计奖励无法得知。

所以$V_π(S)$是一个网络，对一个网络来说，就算输入状态是从来都没有看过的，也可以想办法估计一个值。

训练网络的原则：输入$S_a$,输出的值和$G_a$越接近越好（是一个回归问题）

## 时序差分方法

基于时序差分的方法，不需要玩到游戏结束，在每一步都可以更新$V_π(S_a)$
$$
V_π(s_t)=V_π(s_{t+1})+r_t \tag{6.2}
$$
在训练的时候，我们并不是直接估测$V_π$，而是希望得到的结果$V_π$可以满足式（6.2），我们是这样训练的，我们把 $s_t$输入网络，网络会得到 $V_π(s_t)$，把 $s_{t+1}$ 输入网络会得到 $V_π(s_{t+1})$，$V_π(s_{t})$ 减$V_π(s_{t+1})$的值应该是 $r_t$。我们希望它们相减的损失与 $r_t$接近，训练下去，更新 $V_π$的参数，我们就可以把$V_π$函数学习出来。

# 6.2 动作价值函数

还有一种评论员称为**Q函数**，也称为动作价值函数，他的输入是一个**状态-动作对**，意为在某个状态s时，采取某个动作a，假设都使用策略π，该状态-动作对得到的累计奖励期望值有多大。

<img src="./assets/aeb434096eefbc6d66af952bf1a4a1ad.PNG" alt="aeb434096eefbc6d66af952bf1a4a1ad" style="zoom: 45%;" />

在学习到一个Q函数之后，可以通过贪心策略等，找到一个比π更好的新策略π'，如此循环学习，策略会越来越好

策略更好的概念：对于所有的可能状态S而言都有 $V_{π'}(s)$≥$V_{π}(s)$，也就是说不管在哪一个状态，我们使用π'与环境交互，得到的期望奖励一定不小于使用π的奖励期望

有了Q函数以后，我们把根据式(6.7)决定动作的策略称为 π′
$$
\pi^{\prime}(s) = \underset {a}{\arg \max}Q_{\pi}(s,a) \tag{6.7}
$$

π'一定比π好，在已经学习出π的Q函数的情况下，在状态s下，把所有可能的动作a带入Q函数，令Q函数值最大的a，就是π'将要采取的动作。

Q函数的定义：给定某一个状态s强制采取动作a，**用π继续交互**得到的期望奖励。

但在**随机性策略中**，给定状态s和策略π，并不一定会采取动作π(s)，所以$Q_π（s,π(s)）$不一定会等于$V_π(s)$。



# 6.3 目标网络

<img src="./assets/image-20250907114633710.png" alt="image-20250907114633710" style="zoom:67%;" />

如图所示将右边的Q网络固定住，训练时只更新左边的Q网络的参数，因为右边的Q网络负责产生目标，所以被称为目标网络。最小化左边Q网络和右边的均方误差即可。

在左边的Q网络更新多次之后，再用更新过的Q网络替换目标网络，继续重新训练。

# 6.4 探索

探索—利用窘境，解决方法：**ε-贪心**和**玻尔兹曼探索**(Boltzmann exploration)

ε-贪心：有1-ε的概率按照Q函数决定动作，ε概率随机动作，一般ε设置为0.1

在玻尔兹曼探索中，我们假设对于任意的  $s$, $a$, $Q(s,a)\geqslant 0$  ，因此 $a$  被选中的概率与  $e^{Q(s,a) / T}$  呈正比，即


$$
\pi (a\mid s) = \frac{\mathrm{e}^{Q(s,a) / T}}{\sum_{a'\in A}\mathrm{e}^{Q(s,a') / T}} \tag{6.21}
$$
其中，  $T > 0$  称为温度系数。如果  $T$  很大，所有动作几乎以等概率选择（探索）；如果  $T$  很小，Q值大的动作更容易被选中（利用)；如果  $T$  趋于0，我们就只选择最优动作。

# 6.5 经验回放

基于这部分课件，我来详细讲解经验回放的原理和机制：

## 1. 经验回放的基本架构

**回放缓冲区的构建**

经验回放系统包含三个核心组件：
- **回放缓冲区（Replay Buffer）**：存储历史经验数据(涵盖不同策略，不同状态，不同动作)
- **策略执行**：当前策略π与环境交互
- **Q函数学习**：使用采样经验训练Q网络

<img src="./assets/image-20250907171521709-7254528.png" alt="image-20250907171521709" style="zoom:67%;" />

**数据存储机制**

```python
# 经验存储格式
experience = (s_t, a_t, r_t, s_{t+1})
```
- 每次交互产生一个四元组经验
- 存储容量通常设为5万笔数据
- 采用FIFO（先进先出）策略管理缓冲区

**训练过程**

1. 智能体执行动作，产生新经验
2. 将经验存储到回放缓冲区
3. 从缓冲区随机采样一批经验
4. 使用采样的经验训练Q网络

## 2. 经验回放的两大核心优势

**优势1：提高数据利用效率**

**问题背景**：
- 传统方法中，每个经验只使用一次就被丢弃
- 与环境交互的成本很高（特别是GPU训练时）

**实际效果**：

- 减少与环境交互次数
- 充分利用每一个宝贵的经验数据
- 特别适合GPU并行训练场景

**优势2：增强数据多样性**

**问题背景**：
- 连续时间步的数据高度相关
- 批量中相似数据导致训练效果差

**多样性来源**：

- 不同策略版本的经验混合
- 不同时间点的状态分布
- 随机采样打破序列相关性



## 3. 离策略学习的合理性

**Q: 使用过去策略的经验训练当前Q函数是否合理？**

**A: 完全合理，原因如下：**

1. **目标一致性**：
```python
# 无论经验来源于哪个策略，Q函数的学习目标都是一样的，只是采样了一笔经验
Q_target = r + γ * max_a Q(s', a)  # 贝尔曼方程
```

2. **采样本质**：
```python
# 我们采样的是经验转移，不是完整轨迹
transition = (state, action, reward, next_state)
# 这个转移的价值评估与产生它的策略无关
```

3. **函数逼近视角**：
```python
# Q函数学习的是状态-动作价值映射
Q: S × A → R
# 更多样的(s,a)样本有助于更好的函数逼近
```



## 4.经验回放核心意义

**打破数据的时间相关性，使训练数据满足独立同分布（i.i.d）假设**

具体来说：
- 连续的强化学习经验具有强时间相关性
- 这种相关性会导致神经网络训练不稳定
- 通过随机采样历史经验，打破这种相关性
- 让深度神经网络能够稳定地学习Q函数



# 6.6 深度Q网络

<img src="./assets/image-20250907172645710-7255207.png" alt="image-20250907172645710" style="zoom:50%;" />

在深度Q网络（DQN）中，经验回放、目标网络和Q函数是三个紧密相关的核心组件，它们共同解决了深度强化学习中的稳定性问题。

**Q函数是核心基础**。它估计在给定状态下采取特定动作的期望累积奖励，即Q(s,a)。在DQN中，Q函数由深度神经网络来近似，这个网络接收状态作为输入，输出每个可能动作对应的Q值。

**经验回放解决了数据相关性问题**。在传统的在线学习中，智能体按时间顺序使用经验来更新Q网络，但连续的经验往往高度相关，这会导致网络训练不稳定。经验回放通过将经验(s,a,r,s')存储在缓冲区中，然后随机采样批次来训练Q网络，打破了数据间的时间相关性，使训练更加稳定高效。

**目标网络解决了目标不稳定问题**。在Q学习中，我们用贝尔曼方程来更新Q值，但如果用同一个不断变化的网络既产生当前Q值又产生目标Q值，会造成"追逐移动目标"的问题。目标网络是主网络的一个副本，参数定期从主网络复制过来但在训练期间保持固定，为Q函数更新提供稳定的目标值。

**三者的协同工作机制**是这样的：智能体与环境交互产生的经验被存储到经验回放缓冲区中，训练时从缓冲区随机采样经验批次，使用目标网络计算稳定的目标Q值，然后更新主Q网络的参数。这种设计既保证了数据的多样性，又确保了学习目标的稳定性，从而显著提高了深度Q网络的训练效果和收敛性。
