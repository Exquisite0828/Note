**实验背景：二维栅格地图下的无人机路径规划**

**实验目的：通过实验比较Q-learning和SARSA算法的优劣，比较在大小地图下的表现**



# 第一轮实验：小地图



## 实验条件：

学习率α=0.1，折扣因子γ=0.93，贪婪策略参数epsilon=0.9   每次的回合数episode=3000

到目标点奖励200，碰到障碍物奖励-10，每走一步奖励-1

地图大小为50*30，选择起点为(1,2) 终点（45,13）Q-learning 和SARSA 分别取三次结果

三个图像分别对应每个回合步数，每个回合cost，最后一次路径图

cost代表的是一次episode中，路径上所有状态的Q值的和

cost越来越小的原因：Q的起始值是0，一开始cost偏大（实际cost应该是负值），到后面学习完毕之后，得到了每个点正确的Q值，该Q值要远小于起始值0。

### SARSA结果

#### 第一次

![image-20250823062309832](./assets/image-20250823062309832.png)

![image-20250823062313035](./assets/image-20250823062313035.png)

![image-20250823062322148](./assets/image-20250823062322148.png)

#### 第二次

![image-20250823062447296](./assets/image-20250823062447296.png)

![image-20250823062451865](./assets/image-20250823062451865.png)

![image-20250823062456488](./assets/image-20250823062456488.png)

#### 第三次

![image-20250823062528231](./assets/image-20250823062528231.png)

![image-20250823062532272](./assets/image-20250823062532272.png)

![image-20250823062510558](./assets/image-20250823062510558.png)

### Q-learning结果

#### 第一次

<img src="./assets/image-20250823062537183.png" alt="image-20250823062537183"  />

![image-20250823062542127](./assets/image-20250823062542127.png)

![image-20250823062613541](./assets/image-20250823062613541.png)



#### 第二次

![image-20250823062617929](./assets/image-20250823062617929.png)

![image-20250823062621933](./assets/image-20250823062621933.png)

![image-20250823062625060](./assets/image-20250823062625060.png)





#### 第三次



![image-20250823062630345](./assets/image-20250823062630345.png)

![image-20250823062634219](./assets/image-20250823062634219.png)

![image-20250823062638411](./assets/image-20250823062638411.png)



Q比S相比，平均步数少，cost更大（每个点的Q值更大）

# 第二轮实验：大地图



## 实验条件

学习率α=0.1，折扣因子γ=0.93，贪婪策略参数epsilon=0.9   每次的回合数episode=4500

到目标点奖励200，碰到障碍物奖励-20，每走一步奖励-0.5

地图大小为100*60，选择起点为(10,10) 终点（150,50）Q-learning 和SARSA 分别取三次结果

三个图像分别对应每个回合步数，每个回合cost，最后一次路径图

cost代表的是一次episode中，路径上所有状态的Q值的和

cost越来越小的原因：Q的起始值是0，一开始cost偏大，到后面学习完毕之后，得到了每个点正确的Q值，该Q值要远小于起始值0，呈现减少的趋势，是因为在前中期对一些点的Q值设置不准确，Q值偏大导致的cost偏大。

### SARSA结果

#### SARSA episode=10000

![image-20250823091649292](./assets/image-20250823091649292.png)

![image-20250823091653020](./assets/image-20250823091653020.png)

![image-20250823091656064](./assets/image-20250823091656064.png)

#### 第一次

![image-20250823071439100](./assets/image-20250823071439100.png)

![image-20250823071456275](./assets/image-20250823071456275.png)

![image-20250823071505575](./assets/image-20250823071505575.png)

#### 第二次

![image-20250823072346721](./assets/image-20250823072346721.png)

![image-20250823072356591](./assets/image-20250823072356591.png)

![image-20250823072404272](./assets/image-20250823072404272.png)



#### 第三次

![image-20250823074437579](./assets/image-20250823074437579.png)

![image-20250823074443835](./assets/image-20250823074443835.png)

![image-20250823074447268](./assets/image-20250823074447268.png)



### Q-learning结果

#### 第一次

![image-20250823065633342](./assets/image-20250823065633342.png)

![image-20250823065637893](./assets/image-20250823065637893.png)

![image-20250823065641294](./assets/image-20250823065641294.png)

#### 第二次

![image-20250823070126665](./assets/image-20250823070126665.png)

![image-20250823070129969](./assets/image-20250823070129969.png)

![image-20250823070132987](./assets/image-20250823070132987.png)



#### 第三次

![image-20250823070904306](./assets/image-20250823070904306.png)

![image-20250823070909099](./assets/image-20250823070909099.png)

![image-20250823070912109](C:\Users\11618\AppData\Roaming\Typora\typora-user-images\image-20250823070912109.png)